{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D,Dense,Activation,Flatten,MaxPooling2D,Input,Lambda,Dropout,Conv1D\n",
    "from tensorflow.keras.models import Model,load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar=load_model('cifar1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 9,340,874\n",
      "Trainable params: 9,340,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cifar.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.load('C:\\\\Users\\\\YASH\\\\Cifar-10\\\\cifar_data.npy')\n",
    "train_x=data[0:55000]\n",
    "test_x=data[55000:]\n",
    "label=np.load('C:\\\\Users\\\\YASH\\\\Cifar-10\\\\cifar_label_onehot.npy')\n",
    "train_label=label[0:55000]\n",
    "test_label=label[55000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self,epochs,cd):\n",
    "        count=0\n",
    "        count2=0\n",
    "        norm={}\n",
    "        threshold=0.019\n",
    "#         print(epochs)\n",
    "#         print(cd)\n",
    "        for layers in self.model.layers:\n",
    "            if 'conv2d' in layers.name :#or 'dense' in layers.name:\n",
    "                a=self.model.get_layer(layers.name)\n",
    "                threshold = threshold - 0.002\n",
    "                e=a.get_weights()\n",
    "                if(len(e)>0):\n",
    "                    weights1 = np.asarray(e[0])\n",
    "                    bias1 = np.asarray(e[1])\n",
    "                    dim = weights1.shape[0]*weights1.shape[1]*weights1.shape[2]\n",
    "                    squared_sum = (np.linalg.norm(np.linalg.norm(weights1,axis=(0,1)),axis=0)**2)/dim\n",
    "                    upper_threshold = (squared_sum>threshold)\n",
    "                    lower_threshold = (squared_sum<-threshold)\n",
    "                    weights1 = weights1*(upper_threshold+lower_threshold)\n",
    "                    bias1 = bias1*(upper_threshold+lower_threshold)\n",
    "                    count = np.sum((upper_threshold+lower_threshold)==False)*dim+count\n",
    "                    count2 = np.sum((upper_threshold+lower_threshold)==True)*dim+count2\n",
    "                    e[0] = weights1\n",
    "                    e[1] = bias1\n",
    "                    a.set_weights(e)\n",
    "        print('sparsity: '+str((count/(count2+count))*100))\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "conv2d_10\n",
      "50\n",
      "conv2d_11\n",
      "112\n",
      "conv2d_12\n",
      "153\n",
      "conv2d_13\n",
      "178\n",
      "conv2d_14\n",
      "361\n",
      "conv2d_15\n",
      "201\n",
      "conv2d_16\n",
      "104\n",
      "conv2d_17\n",
      "2\n",
      "conv2d_18\n",
      "57\n",
      "dense_4\n",
      "128\n",
      "dense_5\n",
      "10\n",
      "dense_6\n",
      "sparsity: 68.14419297409923\n"
     ]
    }
   ],
   "source": [
    "cifar=load_model('cifar1.h5')\n",
    "count=0\n",
    "count2=0\n",
    "norm={}\n",
    "threshold=0.005\n",
    "for layers in cifar.layers:\n",
    "    if 'dense' or 'conv2d' or 'conv1d' in layers.name:\n",
    "        a=cifar.get_layer(layers.name)\n",
    "        e=a.get_weights()\n",
    "        if(len(e)>0):\n",
    "            dim = np.prod(e[0].shape[:-1])\n",
    "            squared_sum = np.average(e[0]**2,axis=tuple(np.arange(0,len(e[0].shape)-1)))\n",
    "            upper_threshold = (squared_sum>threshold)\n",
    "            e[0] = e[0]*(upper_threshold)\n",
    "            e[1] = e[1]*(upper_threshold)\n",
    "            print(np.sum(upper_threshold))\n",
    "            a.set_weights(e)\n",
    "            print(layers.name)\n",
    "            count = np.sum((upper_threshold)==False)*dim+count\n",
    "            count2 = np.sum((upper_threshold)==True)*dim+count2\n",
    "print('sparsity: '+str((count/(count2+count))*100))\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "dense_4\n",
      "128\n",
      "dense_5\n",
      "10\n",
      "dense_6\n",
      "sparsity: 43.69230769230769\n"
     ]
    }
   ],
   "source": [
    "cifar=load_model('cifar1.h5')\n",
    "count=0\n",
    "count2=0\n",
    "norm={}\n",
    "threshold=0.005\n",
    "for layers in cifar.layers:\n",
    "    if 'dense' in layers.name:\n",
    "        a=cifar.get_layer(layers.name)\n",
    "#         threshold = threshold - 0.002\n",
    "        e=a.get_weights()\n",
    "        if(len(e)>0):\n",
    "            dim = np.prod(e[0].shape[:-1])\n",
    "            squared_sum = np.average(e[0]**2,axis=tuple(np.arange(0,len(e[0].shape)-1)))\n",
    "            upper_threshold = (squared_sum>threshold)\n",
    "            e[0] = e[0]*(upper_threshold)\n",
    "            e[1] = e[1]*(upper_threshold)\n",
    "            print(np.sum(upper_threshold))\n",
    "            a.set_weights(e)\n",
    "            print(layers.name)\n",
    "            count = np.sum((upper_threshold)==False)*dim+count\n",
    "            count2 = np.sum((upper_threshold)==True)*dim+count2\n",
    "print('sparsity: '+str((count/(count2+count))*100))\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "conv2d_10\n",
      "50\n",
      "conv2d_11\n",
      "112\n",
      "conv2d_12\n",
      "153\n",
      "conv2d_13\n",
      "178\n",
      "conv2d_14\n",
      "361\n",
      "conv2d_15\n",
      "201\n",
      "conv2d_16\n",
      "104\n",
      "conv2d_17\n",
      "2\n",
      "conv2d_18\n",
      "sparsity: 68.36401864069211\n"
     ]
    }
   ],
   "source": [
    "cifar=load_model('cifar1.h5')\n",
    "count=0\n",
    "count2=0\n",
    "norm={}\n",
    "threshold=0.005\n",
    "for layers in cifar.layers:\n",
    "    if 'conv2d' in layers.name:\n",
    "        a=cifar.get_layer(layers.name)\n",
    "#         threshold = threshold - 0.002\n",
    "        e=a.get_weights()\n",
    "        if(len(e)>0):\n",
    "            dim = np.prod(e[0].shape[:-1])\n",
    "            squared_sum = np.average(e[0]**2,axis=tuple(np.arange(0,len(e[0].shape)-1)))\n",
    "            upper_threshold = (squared_sum>threshold)\n",
    "            e[0] = e[0]*(upper_threshold)\n",
    "            e[1] = e[1]*(upper_threshold)\n",
    "            print(np.sum(upper_threshold))\n",
    "            a.set_weights(e)\n",
    "            print(layers.name)\n",
    "            count = np.sum((upper_threshold)==False)*dim+count\n",
    "            count2 = np.sum((upper_threshold)==True)*dim+count2\n",
    "print('sparsity: '+str((count/(count2+count))*100))\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "conv2d_10\n",
      "50\n",
      "conv2d_11\n",
      "112\n",
      "conv2d_12\n",
      "153\n",
      "conv2d_13\n",
      "178\n",
      "conv2d_14\n",
      "361\n",
      "conv2d_15\n",
      "201\n",
      "conv2d_16\n",
      "104\n",
      "conv2d_17\n",
      "2\n",
      "conv2d_18\n",
      "57\n",
      "dense_4\n",
      "128\n",
      "dense_5\n",
      "10\n",
      "dense_6\n",
      "sparsity: 68.14419297409923\n"
     ]
    }
   ],
   "source": [
    "cifar=load_model('cifar1.h5')\n",
    "count=0\n",
    "count2=0\n",
    "norm={}\n",
    "threshold=0.005\n",
    "for layers in cifar.layers:\n",
    "    if 'dense' or 'conv2d' in layers.name:\n",
    "        a=cifar.get_layer(layers.name)\n",
    "#         threshold = threshold - 0.002\n",
    "        e=a.get_weights()\n",
    "        if(len(e)>0):\n",
    "            dim = np.prod(e[0].shape[:-1])\n",
    "            squared_sum = np.average(e[0]**2,axis=tuple(np.arange(0,len(e[0].shape)-1)))\n",
    "            upper_threshold = (squared_sum>threshold)\n",
    "            e[0] = e[0]*(upper_threshold)\n",
    "            e[1] = e[1]*(upper_threshold)\n",
    "            print(np.sum(upper_threshold))\n",
    "            a.set_weights(e)\n",
    "            print(layers.name)\n",
    "            count = np.sum((upper_threshold)==False)*dim+count\n",
    "            count2 = np.sum((upper_threshold)==True)*dim+count2\n",
    "print('sparsity: '+str((count/(count2+count))*100))\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "50\n",
      "112\n",
      "153\n",
      "178\n",
      "361\n",
      "201\n",
      "104\n",
      "2\n",
      "sparsity: 68.36401864069211\n"
     ]
    }
   ],
   "source": [
    "cifar=load_model('cifar1.h5')\n",
    "count=0\n",
    "count2=0\n",
    "norm={}\n",
    "threshold=0.005\n",
    "#         print(epochs)\n",
    "#         print(cd)\n",
    "for layers in cifar.layers:\n",
    "    if 'conv2d' in layers.name :#or 'dense' in layers.name:\n",
    "        a=cifar.get_layer(layers.name)\n",
    "#         threshold = threshold - 0.002\n",
    "        e=a.get_weights()\n",
    "        if(len(e)>0):\n",
    "            weights1 = np.asarray(e[0])\n",
    "            bias1 = np.asarray(e[1])\n",
    "            dim = weights1.shape[0]*weights1.shape[1]*weights1.shape[2]\n",
    "            squared_sum = (np.linalg.norm(np.linalg.norm(weights1,axis=(0,1)),axis=0)**2)/dim\n",
    "            upper_threshold = (squared_sum>threshold)\n",
    "            lower_threshold = (squared_sum<-threshold)\n",
    "            weights1 = weights1*(upper_threshold+lower_threshold)\n",
    "            bias1 = bias1*(upper_threshold+lower_threshold)\n",
    "            print(np.sum(upper_threshold))\n",
    "            count = np.sum((upper_threshold+lower_threshold)==False)*dim+count\n",
    "            count2 = np.sum((upper_threshold+lower_threshold)==True)*dim+count2\n",
    "            e[0] = weights1\n",
    "            e[1] = bias1\n",
    "            a.set_weights(e)\n",
    "print('sparsity: '+str((count/(count2+count))*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(weights1.shape[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 256)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights1.shape[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(weights1**2,axis=tuple(np.arange(0,len(weights1.shape)-1))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple((np.arange(2,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity: 55.03954145142217\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count=0\n",
    "count2=0\n",
    "norm={}\n",
    "threshold=0.019\n",
    "#         print(epochs)\n",
    "#         print(cd)\n",
    "for layers in cifar.layers:\n",
    "    if 'conv2d' in layers.name :#or 'dense' in layers.name:\n",
    "        a=cifar.get_layer(layers.name)\n",
    "        threshold = threshold - 0.002\n",
    "        e=a.get_weights()\n",
    "        if(len(e)>0):\n",
    "            weights1 = np.asarray(e[0])\n",
    "            bias1 = np.asarray(e[1])\n",
    "            dim = weights1.shape[0]*weights1.shape[1]*weights1.shape[2]\n",
    "            squared_sum = (np.linalg.norm(np.linalg.norm(weights1,axis=(0,1)),axis=0)**2)/dim\n",
    "            upper_threshold = (squared_sum>threshold)\n",
    "            lower_threshold = (squared_sum<-threshold)\n",
    "            weights1 = weights1*(upper_threshold+lower_threshold)\n",
    "            bias1 = bias1*(upper_threshold+lower_threshold)\n",
    "            count = np.sum((upper_threshold+lower_threshold)==False)*dim+count\n",
    "            count2 = np.sum((upper_threshold+lower_threshold)==True)*dim+count2\n",
    "            e[0] = weights1\n",
    "            e[1] = bias1\n",
    "            a.set_weights(e)\n",
    "print('sparsity: '+str((count/(count2+count))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def on_train_end():\n",
    "delete_ind=[]\n",
    "layer_details={}\n",
    "allow=0\n",
    "from keras.models import model_from_json\n",
    "import json\n",
    "prunned_model = cifar.to_json()\n",
    "res = json.loads(prunned_model) \n",
    "i=0\n",
    "for layers in cifar.layers:\n",
    "    a=cifar.get_layer(layers.name)\n",
    "    e=a.get_weights()\n",
    "    if 'flatten' in layers.name:\n",
    "        allow=1\n",
    "    if len(e)>0:    # avoid input, maxpooling layers\n",
    "        weights = np.asarray(e[0])\n",
    "        bias = np.asarray(e[1])\n",
    "        if 'conv2d' in layers.name:\n",
    "            weights = np.delete(weights,delete_ind,2)\n",
    "            squared_sum = np.linalg.norm(np.linalg.norm(weights,axis=(0,1)),axis=0)\n",
    "            delete_ind = np.nonzero(squared_sum==0)\n",
    "            weights = np.delete(weights,delete_ind,3)\n",
    "            bias = np.delete(bias,delete_ind,0)\n",
    "            if (layers.name==res['config']['layers'][i]['name']):\n",
    "                res['config']['layers'][i]['config']['filters']=weights.shape[3]                \n",
    "                \n",
    "        if 'dense' in layers.name and allow == 1:\n",
    "            weights = np.delete(weights,delete_ind,0)\n",
    "            allow = 0\n",
    "        layer_details[layers.name] = {'shape': weights.shape,'weights':weights,'bias':bias}\n",
    "        print(weights.shape)\n",
    "\n",
    "    i=i+1\n",
    "res=json.dumps(res)\n",
    "prunned_model = tf.keras.models.model_from_json(res)\n",
    "for layers in prunned_model.layers:\n",
    "    if 'conv2d' in layers.name or 'dense' in layers.name :#or 'dense' in layers.name:\n",
    "        a=prunned_model.get_layer(layers.name)\n",
    "        e=a.get_weights()\n",
    "        e[0] = layer_details[layers.name]['weights']\n",
    "        e[1] = layer_details[layers.name]['bias']\n",
    "        a.set_weights(e)        \n",
    "        \n",
    "prunned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_details['conv2d_55']['weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "json_model = cifar.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = json.loads(json_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['config']['layers'][2]['config']['filters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layers in cifar.layers:\n",
    "    print(layers.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar=load_model('cifar1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_j = tf.keras.models.model_from_json(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_j.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_j = tf.keras.models.model_from_json(json_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prunned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt=Adam(lr=0.0001/2)\n",
    "cifar.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "cifar.fit(train_x,train_label,epochs=1,validation_split=0.05,batch_size=64)#,callbacks=[MyCustomCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prunned_model.save('befquan.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=np.linalg.norm(np.linalg.norm(c,axis=(0,1)),axis=0)/(c.shape[0]+c.shape[1]+c.shape[2])\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=Adam(lr=0.001)\n",
    "quan_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prunned_model.evaluate(x=test_x,y=test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quan_model.evaluate(x=test_x,y=test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 2s 452us/sample - loss: 0.5680 - accuracy: 0.8320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.568046052646637, 0.832]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar.evaluate(x=test_x,y=test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar.evaluate(x=train_x,y=train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "quan_model = cifar.to_json()\n",
    "res = json.loads(quan_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,22):\n",
    "    res['config']['layers'][i]['config']['dtype']='float16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=json.dumps(res)\n",
    "quan_model = tf.keras.models.model_from_json(res)\n",
    "for layers in quan_model.layers:\n",
    "    if 'conv2d' in layers.name or 'dense' in layers.name :#or 'dense' in layers.name:\n",
    "        a=quan_model.get_layer(layers.name)\n",
    "        e=a.get_weights()\n",
    "        e[0] = layer_details[layers.name]['weights']\n",
    "        e[1] = layer_details[layers.name]['bias']\n",
    "        a.set_weights(e)        \n",
    "        \n",
    "quan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quan123_model = quan_model.to_json()\n",
    "res = json.loads(quan123_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=Adam(lr=0.001)\n",
    "quan_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quan_model.evaluate(x=test_x,y=test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quan_model.save('aftquan.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=cifar.get_layer('conv2d_15')\n",
    "e=a.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 256, 512)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[0,0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=cifar.get_layer('dense_6')\n",
    "e=a.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e[0].shape[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(e[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_name': 'Model',\n",
       " 'config': {'name': 'model_2',\n",
       "  'layers': [{'class_name': 'InputLayer',\n",
       "    'config': {'batch_input_shape': [None, 32, 32, 3],\n",
       "     'dtype': 'float32',\n",
       "     'sparse': False,\n",
       "     'ragged': False,\n",
       "     'name': 'input_2'},\n",
       "    'name': 'input_2',\n",
       "    'inbound_nodes': []},\n",
       "   {'class_name': 'Lambda',\n",
       "    'config': {'name': 'lambda_2',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'function': ['4wEAAAAAAAAAAQAAAAIAAABDAAAAcwwAAAB8AGQBGwBkAhgAUwApA07p/wAAAGcAAAAAAADgP6kA\\nKQHaAXhyAgAAAHICAAAA+h48aXB5dGhvbi1pbnB1dC05LTFhZTE4OWM4NzEwMT7aCDxsYW1iZGE+\\nAQAAAPMAAAAA\\n',\n",
       "      None,\n",
       "      None],\n",
       "     'function_type': 'lambda',\n",
       "     'module': 'tensorflow.python.keras.layers.core',\n",
       "     'output_shape': None,\n",
       "     'output_shape_type': 'raw',\n",
       "     'output_shape_module': None,\n",
       "     'arguments': {}},\n",
       "    'name': 'lambda_2',\n",
       "    'inbound_nodes': [[['input_2', 0, 0, {}]]]},\n",
       "   {'class_name': 'Conv2D',\n",
       "    'config': {'name': 'conv2d_10',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'filters': 64,\n",
       "     'kernel_size': [3, 3],\n",
       "     'strides': [1, 1],\n",
       "     'padding': 'same',\n",
       "     'data_format': 'channels_last',\n",
       "     'dilation_rate': [1, 1],\n",
       "     'activation': 'relu',\n",
       "     'use_bias': True,\n",
       "     'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "      'config': {'scale': 1.0,\n",
       "       'mode': 'fan_avg',\n",
       "       'distribution': 'uniform',\n",
       "       'seed': None}},\n",
       "     'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'kernel_regularizer': None,\n",
       "     'bias_regularizer': None,\n",
       "     'activity_regularizer': None,\n",
       "     'kernel_constraint': None,\n",
       "     'bias_constraint': None},\n",
       "    'name': 'conv2d_10',\n",
       "    'inbound_nodes': [[['lambda_2', 0, 0, {}]]]},\n",
       "   {'class_name': 'Conv2D',\n",
       "    'config': {'name': 'conv2d_11',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'filters': 64,\n",
       "     'kernel_size': [3, 3],\n",
       "     'strides': [1, 1],\n",
       "     'padding': 'same',\n",
       "     'data_format': 'channels_last',\n",
       "     'dilation_rate': [1, 1],\n",
       "     'activation': 'relu',\n",
       "     'use_bias': True,\n",
       "     'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "      'config': {'scale': 1.0,\n",
       "       'mode': 'fan_avg',\n",
       "       'distribution': 'uniform',\n",
       "       'seed': None}},\n",
       "     'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'kernel_regularizer': None,\n",
       "     'bias_regularizer': None,\n",
       "     'activity_regularizer': None,\n",
       "     'kernel_constraint': None,\n",
       "     'bias_constraint': None},\n",
       "    'name': 'conv2d_11',\n",
       "    'inbound_nodes': [[['conv2d_10', 0, 0, {}]]]},\n",
       "   {'class_name': 'MaxPooling2D',\n",
       "    'config': {'name': 'max_pooling2d_6',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'pool_size': [2, 2],\n",
       "     'padding': 'valid',\n",
       "     'strides': [2, 2],\n",
       "     'data_format': 'channels_last'},\n",
       "    'name': 'max_pooling2d_6',\n",
       "    'inbound_nodes': [[['conv2d_11', 0, 0, {}]]]},\n",
       "   {'class_name': 'Conv2D',\n",
       "    'config': {'name': 'conv2d_12',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'filters': 128,\n",
       "     'kernel_size': [3, 3],\n",
       "     'strides': [1, 1],\n",
       "     'padding': 'same',\n",
       "     'data_format': 'channels_last',\n",
       "     'dilation_rate': [1, 1],\n",
       "     'activation': 'relu',\n",
       "     'use_bias': True,\n",
       "     'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "      'config': {'scale': 1.0,\n",
       "       'mode': 'fan_avg',\n",
       "       'distribution': 'uniform',\n",
       "       'seed': None}},\n",
       "     'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'kernel_regularizer': None,\n",
       "     'bias_regularizer': None,\n",
       "     'activity_regularizer': None,\n",
       "     'kernel_constraint': None,\n",
       "     'bias_constraint': None},\n",
       "    'name': 'conv2d_12',\n",
       "    'inbound_nodes': [[['max_pooling2d_6', 0, 0, {}]]]},\n",
       "   {'class_name': 'Dropout',\n",
       "    'config': {'name': 'dropout_3',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'rate': 0.5,\n",
       "     'noise_shape': None,\n",
       "     'seed': None},\n",
       "    'name': 'dropout_3',\n",
       "    'inbound_nodes': [[['conv2d_12', 0, 0, {}]]]},\n",
       "   {'class_name': 'MaxPooling2D',\n",
       "    'config': {'name': 'max_pooling2d_7',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'pool_size': [2, 2],\n",
       "     'padding': 'valid',\n",
       "     'strides': [2, 2],\n",
       "     'data_format': 'channels_last'},\n",
       "    'name': 'max_pooling2d_7',\n",
       "    'inbound_nodes': [[['dropout_3', 0, 0, {}]]]},\n",
       "   {'class_name': 'Conv2D',\n",
       "    'config': {'name': 'conv2d_13',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'filters': 256,\n",
       "     'kernel_size': [3, 3],\n",
       "     'strides': [1, 1],\n",
       "     'padding': 'same',\n",
       "     'data_format': 'channels_last',\n",
       "     'dilation_rate': [1, 1],\n",
       "     'activation': 'relu',\n",
       "     'use_bias': True,\n",
       "     'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "      'config': {'scale': 1.0,\n",
       "       'mode': 'fan_avg',\n",
       "       'distribution': 'uniform',\n",
       "       'seed': None}},\n",
       "     'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'kernel_regularizer': None,\n",
       "     'bias_regularizer': None,\n",
       "     'activity_regularizer': None,\n",
       "     'kernel_constraint': None,\n",
       "     'bias_constraint': None},\n",
       "    'name': 'conv2d_13',\n",
       "    'inbound_nodes': [[['max_pooling2d_7', 0, 0, {}]]]},\n",
       "   {'class_name': 'Conv2D',\n",
       "    'config': {'name': 'conv2d_14',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'filters': 256,\n",
       "     'kernel_size': [3, 3],\n",
       "     'strides': [1, 1],\n",
       "     'padding': 'same',\n",
       "     'data_format': 'channels_last',\n",
       "     'dilation_rate': [1, 1],\n",
       "     'activation': 'relu',\n",
       "     'use_bias': True,\n",
       "     'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "      'config': {'scale': 1.0,\n",
       "       'mode': 'fan_avg',\n",
       "       'distribution': 'uniform',\n",
       "       'seed': None}},\n",
       "     'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'kernel_regularizer': None,\n",
       "     'bias_regularizer': None,\n",
       "     'activity_regularizer': None,\n",
       "     'kernel_constraint': None,\n",
       "     'bias_constraint': None},\n",
       "    'name': 'conv2d_14',\n",
       "    'inbound_nodes': [[['conv2d_13', 0, 0, {}]]]},\n",
       "   {'class_name': 'Dropout',\n",
       "    'config': {'name': 'dropout_4',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'rate': 0.5,\n",
       "     'noise_shape': None,\n",
       "     'seed': None},\n",
       "    'name': 'dropout_4',\n",
       "    'inbound_nodes': [[['conv2d_14', 0, 0, {}]]]},\n",
       "   {'class_name': 'MaxPooling2D',\n",
       "    'config': {'name': 'max_pooling2d_8',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'pool_size': [2, 2],\n",
       "     'padding': 'valid',\n",
       "     'strides': [2, 2],\n",
       "     'data_format': 'channels_last'},\n",
       "    'name': 'max_pooling2d_8',\n",
       "    'inbound_nodes': [[['dropout_4', 0, 0, {}]]]},\n",
       "   {'class_name': 'Conv2D',\n",
       "    'config': {'name': 'conv2d_15',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'filters': 512,\n",
       "     'kernel_size': [3, 3],\n",
       "     'strides': [1, 1],\n",
       "     'padding': 'same',\n",
       "     'data_format': 'channels_last',\n",
       "     'dilation_rate': [1, 1],\n",
       "     'activation': 'relu',\n",
       "     'use_bias': True,\n",
       "     'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "      'config': {'scale': 1.0,\n",
       "       'mode': 'fan_avg',\n",
       "       'distribution': 'uniform',\n",
       "       'seed': None}},\n",
       "     'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'kernel_regularizer': None,\n",
       "     'bias_regularizer': None,\n",
       "     'activity_regularizer': None,\n",
       "     'kernel_constraint': None,\n",
       "     'bias_constraint': None},\n",
       "    'name': 'conv2d_15',\n",
       "    'inbound_nodes': [[['max_pooling2d_8', 0, 0, {}]]]},\n",
       "   {'class_name': 'Conv2D',\n",
       "    'config': {'name': 'conv2d_16',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'filters': 512,\n",
       "     'kernel_size': [3, 3],\n",
       "     'strides': [1, 1],\n",
       "     'padding': 'same',\n",
       "     'data_format': 'channels_last',\n",
       "     'dilation_rate': [1, 1],\n",
       "     'activation': 'relu',\n",
       "     'use_bias': True,\n",
       "     'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "      'config': {'scale': 1.0,\n",
       "       'mode': 'fan_avg',\n",
       "       'distribution': 'uniform',\n",
       "       'seed': None}},\n",
       "     'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'kernel_regularizer': None,\n",
       "     'bias_regularizer': None,\n",
       "     'activity_regularizer': None,\n",
       "     'kernel_constraint': None,\n",
       "     'bias_constraint': None},\n",
       "    'name': 'conv2d_16',\n",
       "    'inbound_nodes': [[['conv2d_15', 0, 0, {}]]]},\n",
       "   {'class_name': 'MaxPooling2D',\n",
       "    'config': {'name': 'max_pooling2d_9',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'pool_size': [2, 2],\n",
       "     'padding': 'valid',\n",
       "     'strides': [2, 2],\n",
       "     'data_format': 'channels_last'},\n",
       "    'name': 'max_pooling2d_9',\n",
       "    'inbound_nodes': [[['conv2d_16', 0, 0, {}]]]},\n",
       "   {'class_name': 'Conv2D',\n",
       "    'config': {'name': 'conv2d_17',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'filters': 512,\n",
       "     'kernel_size': [3, 3],\n",
       "     'strides': [1, 1],\n",
       "     'padding': 'same',\n",
       "     'data_format': 'channels_last',\n",
       "     'dilation_rate': [1, 1],\n",
       "     'activation': 'relu',\n",
       "     'use_bias': True,\n",
       "     'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "      'config': {'scale': 1.0,\n",
       "       'mode': 'fan_avg',\n",
       "       'distribution': 'uniform',\n",
       "       'seed': None}},\n",
       "     'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'kernel_regularizer': None,\n",
       "     'bias_regularizer': None,\n",
       "     'activity_regularizer': None,\n",
       "     'kernel_constraint': None,\n",
       "     'bias_constraint': None},\n",
       "    'name': 'conv2d_17',\n",
       "    'inbound_nodes': [[['max_pooling2d_9', 0, 0, {}]]]},\n",
       "   {'class_name': 'Conv2D',\n",
       "    'config': {'name': 'conv2d_18',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'filters': 512,\n",
       "     'kernel_size': [3, 3],\n",
       "     'strides': [1, 1],\n",
       "     'padding': 'same',\n",
       "     'data_format': 'channels_last',\n",
       "     'dilation_rate': [1, 1],\n",
       "     'activation': 'relu',\n",
       "     'use_bias': True,\n",
       "     'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "      'config': {'scale': 1.0,\n",
       "       'mode': 'fan_avg',\n",
       "       'distribution': 'uniform',\n",
       "       'seed': None}},\n",
       "     'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'kernel_regularizer': None,\n",
       "     'bias_regularizer': None,\n",
       "     'activity_regularizer': None,\n",
       "     'kernel_constraint': None,\n",
       "     'bias_constraint': None},\n",
       "    'name': 'conv2d_18',\n",
       "    'inbound_nodes': [[['conv2d_17', 0, 0, {}]]]},\n",
       "   {'class_name': 'MaxPooling2D',\n",
       "    'config': {'name': 'max_pooling2d_10',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'pool_size': [2, 2],\n",
       "     'padding': 'valid',\n",
       "     'strides': [2, 2],\n",
       "     'data_format': 'channels_last'},\n",
       "    'name': 'max_pooling2d_10',\n",
       "    'inbound_nodes': [[['conv2d_18', 0, 0, {}]]]},\n",
       "   {'class_name': 'Flatten',\n",
       "    'config': {'name': 'flatten_2',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'data_format': 'channels_last'},\n",
       "    'name': 'flatten_2',\n",
       "    'inbound_nodes': [[['max_pooling2d_10', 0, 0, {}]]]},\n",
       "   {'class_name': 'Dense',\n",
       "    'config': {'name': 'dense_4',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'units': 128,\n",
       "     'activation': 'relu',\n",
       "     'use_bias': True,\n",
       "     'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "      'config': {'scale': 1.0,\n",
       "       'mode': 'fan_avg',\n",
       "       'distribution': 'uniform',\n",
       "       'seed': None}},\n",
       "     'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'kernel_regularizer': None,\n",
       "     'bias_regularizer': None,\n",
       "     'activity_regularizer': None,\n",
       "     'kernel_constraint': None,\n",
       "     'bias_constraint': None},\n",
       "    'name': 'dense_4',\n",
       "    'inbound_nodes': [[['flatten_2', 0, 0, {}]]]},\n",
       "   {'class_name': 'Dense',\n",
       "    'config': {'name': 'dense_5',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'units': 128,\n",
       "     'activation': 'relu',\n",
       "     'use_bias': True,\n",
       "     'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "      'config': {'scale': 1.0,\n",
       "       'mode': 'fan_avg',\n",
       "       'distribution': 'uniform',\n",
       "       'seed': None}},\n",
       "     'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'kernel_regularizer': None,\n",
       "     'bias_regularizer': None,\n",
       "     'activity_regularizer': None,\n",
       "     'kernel_constraint': None,\n",
       "     'bias_constraint': None},\n",
       "    'name': 'dense_5',\n",
       "    'inbound_nodes': [[['dense_4', 0, 0, {}]]]},\n",
       "   {'class_name': 'Dense',\n",
       "    'config': {'name': 'dense_6',\n",
       "     'trainable': True,\n",
       "     'dtype': 'float32',\n",
       "     'units': 10,\n",
       "     'activation': 'softmax',\n",
       "     'use_bias': True,\n",
       "     'kernel_initializer': {'class_name': 'VarianceScaling',\n",
       "      'config': {'scale': 1.0,\n",
       "       'mode': 'fan_avg',\n",
       "       'distribution': 'uniform',\n",
       "       'seed': None}},\n",
       "     'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
       "     'kernel_regularizer': None,\n",
       "     'bias_regularizer': None,\n",
       "     'activity_regularizer': {'class_name': 'L1L2',\n",
       "      'config': {'l1': 0.0, 'l2': 0.009999999776482582}},\n",
       "     'kernel_constraint': None,\n",
       "     'bias_constraint': None},\n",
       "    'name': 'dense_6',\n",
       "    'inbound_nodes': [[['dense_5', 0, 0, {}]]]}],\n",
       "  'input_layers': [['input_2', 0, 0]],\n",
       "  'output_layers': [['dense_6', 0, 0]]},\n",
       " 'keras_version': '2.2.4-tf',\n",
       " 'backend': 'tensorflow'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=Input((128,128))\n",
    "x=Conv1D(100,3)(inp)\n",
    "x=Conv1D(50,5)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 128, 128)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 126, 100)          38500     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 122, 50)           25050     \n",
      "=================================================================\n",
      "Total params: 63,550\n",
      "Trainable params: 63,550\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod=Model(inp,x)\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=mod.get_layer('conv1d_2')\n",
    "b=a.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 100, 50)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final compression\n",
    "delete_ind=[]\n",
    "layer_details={}\n",
    "allow=0\n",
    "from keras.models import model_from_json\n",
    "import json\n",
    "prunned_model = rnn.to_json() # converting model into json\n",
    "\n",
    "res = json.loads(prunned_model) \n",
    "i=0\n",
    "\n",
    "for layers in rnn.layers:\n",
    "    a=rnn.get_layer(layers.name) # getting weights\n",
    "    e=a.get_weights()\n",
    "    \n",
    "    #checking for layers with none parameters\n",
    "    if len(e)>0:\n",
    "        if 'lstm' in layers.name:  # for lstm layer \n",
    "            e[0] = np.delete(e[0],delete_ind,0)  # removing dimension from previous layers \n",
    "            avg =np.average(e[1]**2,axis=0)+np.average(e[0]**2,axis=0) # avg to find which filters to remove\n",
    "            weights1=0\n",
    "            units = e[1].shape[0]\n",
    "            \n",
    "            for j in range(0,4):\n",
    "                weights1=weights1+avg[units*j:units*(j+1)]\n",
    "            delete_ind = np.nonzero(weights1==0)\n",
    "            \n",
    "            #removing weights \n",
    "            c= np.concatenate((delete_ind[0],delete_ind[0]+units,delete_ind[0]+units*2,delete_ind[0]+units*3))\n",
    "            e[0] = np.delete(e[0],c,1)\n",
    "            e[1] = np.delete(e[1],delete_ind,0)\n",
    "            e[1] = np.delete(e[1],c,1)\n",
    "            e[2] = np.delete(e[2],c,0)\n",
    "            \n",
    "            # changing number of filters in model\n",
    "            if (layers.name==res['config']['layers'][i]['name']):\n",
    "                res['config']['layers'][i]['config']['units']=e[1].shape[0]\n",
    "                \n",
    "            # repeating for the dense layer \n",
    "        if 'dense' in layers.name or 'conv2d' in layers.name or 'conv1d' in layers.name:\n",
    "            e[0] = np.delete(e[0],delete_ind,(len(e[0].shape)-2))\n",
    "            avg = np.average(e[0]**2,axis=tuple(np.arange(0,len(e[0].shape)-1)))\n",
    "            delete_ind = np.nonzero(avg==0)\n",
    "            e[0] = np.delete(e[0],delete_ind,(len(e[0].shape)-1))\n",
    "            e[1] = np.delete(e[1],delete_ind,0)\n",
    "            if (layers.name==res['config']['layers'][i]['name']):\n",
    "                res['config']['layers'][i]['config']['units']=e[1].shape[0]\n",
    "            \n",
    "        layer_details[layers.name] = {'weights':e}\n",
    "    i=i+1\n",
    "res=json.dumps(res)\n",
    "prunned_model = tf.keras.models.model_from_json(res)\n",
    "for layers in prunned_model.layers:\n",
    "    if 'lstm' in layers.name or 'dense' in layers.name:\n",
    "        a=prunned_model.get_layer(layers.name)\n",
    "        e=a.get_weights()\n",
    "        e=layer_details[layers.name]['weights']\n",
    "        a.set_weights(e)        \n",
    "        \n",
    "prunned_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
